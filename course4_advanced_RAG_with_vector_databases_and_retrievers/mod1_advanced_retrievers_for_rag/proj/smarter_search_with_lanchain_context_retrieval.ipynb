{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c8fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm-watsonx-ai==1.1.2\n",
      "  Downloading ibm_watsonx_ai-1.1.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: requests in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2.32.2)\n",
      "Requirement already satisfied: urllib3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2.3.0)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2.1.4)\n",
      "Requirement already satisfied: certifi in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (2025.11.12)\n",
      "Requirement already satisfied: lomond in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (0.3.3)\n",
      "Requirement already satisfied: tabulate in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (0.9.0)\n",
      "Requirement already satisfied: packaging in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (23.2)\n",
      "Collecting ibm-cos-sdk<2.14.0,>=2.12.0 (from ibm-watsonx-ai==1.1.2)\n",
      "  Using cached ibm_cos_sdk-2.13.6-py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai==1.1.2) (8.7.0)\n",
      "Collecting ibm-cos-sdk-core==2.13.6 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2)\n",
      "  Using cached ibm_cos_sdk_core-2.13.6-py3-none-any.whl\n",
      "Collecting ibm-cos-sdk-s3transfer==2.13.6 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2)\n",
      "  Using cached ibm_cos_sdk_s3transfer-2.13.6-py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.1.2) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.1.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.1.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.9.0->ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.1.2) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests->ibm-watsonx-ai==1.1.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests->ibm-watsonx-ai==1.1.2) (3.11)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from importlib-metadata->ibm-watsonx-ai==1.1.2) (3.23.0)\n",
      "Downloading ibm_watsonx_ai-1.1.2-py3-none-any.whl (966 kB)\n",
      "   ---------------------------------------- 0.0/966.6 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 524.3/966.6 kB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 966.6/966.6 kB 5.0 MB/s  0:00:00\n",
      "Installing collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watsonx-ai\n",
      "\n",
      "  Attempting uninstall: ibm-cos-sdk-core\n",
      "\n",
      "    Found existing installation: ibm-cos-sdk-core 2.14.1\n",
      "\n",
      "    Uninstalling ibm-cos-sdk-core-2.14.1:\n",
      "\n",
      "      Successfully uninstalled ibm-cos-sdk-core-2.14.1\n",
      "\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "  Attempting uninstall: ibm-cos-sdk-s3transfer\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "    Found existing installation: ibm-cos-sdk-s3transfer 2.14.1\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "    Uninstalling ibm-cos-sdk-s3transfer-2.14.1:\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "      Successfully uninstalled ibm-cos-sdk-s3transfer-2.14.1\n",
      "   ---------------------------------------- 0/4 [ibm-cos-sdk-core]\n",
      "   ---------- ----------------------------- 1/4 [ibm-cos-sdk-s3transfer]\n",
      "   ---------- ----------------------------- 1/4 [ibm-cos-sdk-s3transfer]\n",
      "  Attempting uninstall: ibm-cos-sdk\n",
      "   ---------- ----------------------------- 1/4 [ibm-cos-sdk-s3transfer]\n",
      "    Found existing installation: ibm-cos-sdk 2.14.1\n",
      "   ---------- ----------------------------- 1/4 [ibm-cos-sdk-s3transfer]\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "    Uninstalling ibm-cos-sdk-2.14.1:\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "      Successfully uninstalled ibm-cos-sdk-2.14.1\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "  Attempting uninstall: ibm-watsonx-ai\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "    Found existing installation: ibm_watsonx_ai 1.4.9\n",
      "   -------------------- ------------------- 2/4 [ibm-cos-sdk]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "    Uninstalling ibm_watsonx_ai-1.4.9:\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "      Successfully uninstalled ibm_watsonx_ai-1.4.9\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ------------------------------ --------- 3/4 [ibm-watsonx-ai]\n",
      "   ---------------------------------------- 4/4 [ibm-watsonx-ai]\n",
      "\n",
      "Successfully installed ibm-cos-sdk-2.13.6 ibm-cos-sdk-core-2.13.6 ibm-cos-sdk-s3transfer-2.13.6 ibm-watsonx-ai-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-ibm 0.1.4 requires ibm-watsonx-ai<0.3.0,>=0.2.6, but you have ibm-watsonx-ai 1.1.2 which is incompatible.\n",
      "langchain-ibm 0.1.4 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.2.43 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.2.1\n",
      "  Downloading langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (3.13.2)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain==0.2.1) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.22.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (4.12.0)\n",
      "Requirement already satisfied: certifi in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.1) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests<3,>=2->langchain==0.2.1) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.1) (3.3.0)\n",
      "Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "   ---------------------------------------- 0.0/973.5 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 786.4/973.5 kB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 973.5/973.5 kB 5.0 MB/s  0:00:00\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.11\n",
      "    Uninstalling langchain-0.2.11:\n",
      "      Successfully uninstalled langchain-0.2.11\n",
      "Successfully installed langchain-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.2.10 requires langchain<0.3.0,>=0.2.9, but you have langchain 0.2.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ibm==0.1.11\n",
      "  Downloading langchain_ibm-0.1.11-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: ibm-watsonx-ai<2.0.0,>=1.0.8 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-ibm==0.1.11) (1.1.2)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-ibm==0.1.11) (0.2.43)\n",
      "Requirement already satisfied: requests in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.32.2)\n",
      "Requirement already satisfied: urllib3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.3.0)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.1.4)\n",
      "Requirement already satisfied: certifi in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2025.11.12)\n",
      "Requirement already satisfied: lomond in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (0.3.3)\n",
      "Requirement already satisfied: tabulate in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (0.9.0)\n",
      "Requirement already satisfied: packaging in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (23.2)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.13.6)\n",
      "Requirement already satisfied: importlib-metadata in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (8.7.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.13.6)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.13.6)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (6.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (0.16.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain-ibm==0.1.11) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.9.0->ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (3.4.4)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from importlib-metadata->ibm-watsonx-ai<2.0.0,>=1.0.8->langchain-ibm==0.1.11) (3.23.0)\n",
      "Downloading langchain_ibm-0.1.11-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: langchain-ibm\n",
      "  Attempting uninstall: langchain-ibm\n",
      "    Found existing installation: langchain-ibm 0.1.4\n",
      "    Uninstalling langchain-ibm-0.1.4:\n",
      "      Successfully uninstalled langchain-ibm-0.1.4\n",
      "Successfully installed langchain-ibm-0.1.11\n",
      "Collecting langchain-community==0.2.1\n",
      "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (0.2.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (0.2.43)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (2.32.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-community==0.2.1) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (4.12.0)\n",
      "Requirement already satisfied: certifi in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.2.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.2.1) (2.3.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.1) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (1.1.0)\n",
      "Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.1 MB/s  0:00:00\n",
      "Installing collected packages: langchain-community\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.10\n",
      "    Uninstalling langchain-community-0.2.10:\n",
      "      Successfully uninstalled langchain-community-0.2.10\n",
      "Successfully installed langchain-community-0.2.1\n",
      "Collecting chromadb==0.4.24\n",
      "  Using cached chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.28 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (2.32.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.38.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (4.15.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (3.8.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.39.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (34.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (6.0.3)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from chromadb==0.4.24) (3.11.4)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from build>=1.0.3->chromadb==0.4.24) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb==0.4.24) (0.45.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.24) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb==0.4.24) (2.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from starlette<0.46.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.24) (4.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.24) (3.11)\n",
      "Requirement already satisfied: certifi>=14.05.14 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2025.11.12)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (25.9.23)\n",
      "Requirement already satisfied: protobuf in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (6.33.1)\n",
      "Requirement already satisfied: sympy in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.24) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.24) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb==0.4.24) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.60b0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.60b0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.60b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.60b0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (0.60b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-instrumentation==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (1.17.3)\n",
      "Requirement already satisfied: asgiref~=3.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.60b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (3.11.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.24) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.4.24) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests>=2.28->chromadb==0.4.24) (3.4.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb==0.4.24) (0.36.0)\n",
      "Requirement already satisfied: filelock in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.24) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.24) (2025.12.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.24) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.24) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb==0.4.24) (14.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (0.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.16.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (1.2.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (11.0.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.24) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.24) (3.5.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.4.24) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.24) (1.3.0)\n",
      "Using cached chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "Installing collected packages: chromadb\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 1.0.12\n",
      "    Uninstalling chromadb-1.0.12:\n",
      "      Successfully uninstalled chromadb-1.0.12\n",
      "Successfully installed chromadb-0.4.24\n",
      "Requirement already satisfied: pypdf==4.3.1 in d:\\ibm_rag_agentic_ai_course\\venv\\lib\\site-packages (4.3.1)\n",
      "Collecting lark==1.1.9\n",
      "  Downloading lark-1.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: lark\n",
      "Successfully installed lark-1.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "!pip install \"ibm-watsonx-ai==1.1.2\"\n",
    "!pip install \"langchain==0.2.1\"\n",
    "!pip install \"langchain-ibm==0.1.11\"\n",
    "!pip install \"langchain-community==0.2.1\"\n",
    "!pip install \"chromadb==0.4.24\"\n",
    "!pip install \"pypdf==4.3.1\"\n",
    "!pip install \"lark==1.1.9\"\n",
    "!pip install 'posthog<6.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c433c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725c1b3",
   "metadata": {},
   "source": [
    "### Creating a retriever model\n",
    "\n",
    "#### The following steps are involved to create a retriever model using LangChain:\n",
    "\n",
    " Building LLMs\n",
    "\n",
    " Splitting documents into chunks\n",
    "\n",
    " Building an embedding model\n",
    "\n",
    " Retrieving related knowledge from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fca5e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM connection successful: Yes, I can respond. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Build the LLM\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not openrouter_api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")\n",
    "\n",
    "# Initialize LLM via OpenRouter\n",
    "llm = ChatOpenAI(\n",
    "    model=\"mistralai/mistral-small-3.1-24b-instruct:free\",\n",
    "    api_key=openrouter_api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "# Test LLM connection\n",
    "try:\n",
    "    response = llm.invoke(\"Hello, can you respond?\")\n",
    "    print(\"LLM connection successful:\", response.content)\n",
    "except Exception as e:\n",
    "    print(\"LLM connection failed:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Text Splitter\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_splitter(data, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60e9ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Create the Embedding Model\n",
    "\n",
    "HFembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c141339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 15660  100 15660    0     0   9339      0  0:00:01  0:00:01 --:--:--  9349\n"
     ]
    }
   ],
   "source": [
    "# Download the sample document\n",
    "\n",
    "# !curl \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MZ9z1lm-Ui3YBp3SYWLTAQ/companypolicies.txt\" > companypolicies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d689900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Text Loader to load the document\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"companypolicies.txt\")\n",
    "txt_data = loader.load()\n",
    "\n",
    "# txt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split txt_data into chunks. chunk_size = 200, chunk_overlap = 20 has been set.\n",
    "chunks_txt = text_splitter(txt_data, chunk_size=200, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea9b1016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the embeddings to ChromaDB\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create and store the embeddings in ChromaDB\n",
    "vectordb = Chroma.from_documents(documents=chunks_txt, embedding=HFembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20f591e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'companypolicies.txt'}, page_content='3.\\tInternet and Email Policy'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Our Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Our Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Confidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple similarity search\n",
    "query = \"email policy\"\n",
    "# retriever = vectordb.as_retriever(search_kwargs={\"k\":1}) # to limit number of docs retrieved\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f2af6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'companypolicies.txt'}, page_content='3.\\tInternet and Email Policy'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Confidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Review of Policy: This policy will be reviewed periodically to ensure its alignment with evolving legal requirements and best practices for maintaining a healthy and safe workplace.'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='individual found to be in violation of this policy.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAXIMUM MARGINAL RELEVANCE (MMR) RETRIEVER\n",
    "# retriever_mmr = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3, \"fetch_k\":5})\n",
    "\n",
    "retriever_mmr = vectordb.as_retriever(search_type=\"mmr\")\n",
    "docs_mmr = retriever_mmr.invoke(query)\n",
    "docs_mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'companypolicies.txt'}, page_content='3.\\tInternet and Email Policy'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Our Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Our Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and'),\n",
       " Document(metadata={'source': 'companypolicies.txt'}, page_content='Confidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity Score Threshold Retriever\n",
    "# You can also set a retrieval method that defines a similarity score threshold, returning only documents with a score above that threshold.\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.4}\n",
    ")\n",
    "\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc1384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'langchain_paper.pdf', 'page': 1}, page_content='LangChain helps us to unlock the ability to harness the \\nLLMs immense potential in tasks such as document analysis, \\nchatbot development, code analysis, and countless other \\napplications. Whether your desire is to unlock deeper natural \\nlanguage understanding , enhance data, or circumvent \\nlanguage barriers through translation, LangChain is ready to \\nprovide the tools and programming support you need to do \\nwithout it that it is not only difficult but also fresh for you . Its \\ncore functionalities encompass:  \\n1. Context -Aware Capabilities: LangChain facilitates the \\ndevelopment of applications that are inherently \\ncontext -aware. This means that these applications can \\nconnect to a language model and draw from various \\nsources of context, such as prompt instructions, a  few-\\nshot examples, or existing content, to ground their \\nresponses effectively.  \\n2. Reasoning Abilities: LangChain equips applications \\nwith the capacity to reason effectively. By relying on a \\nlanguage model, these applications can make informed \\ndecisions about how to respond based on the provided \\ncontext and determine the appropriate acti ons to take.  \\nLangChain offers several key value propositions:  \\nModular Components: It provides abstractions that \\nsimplify working with language models, along with a \\ncomprehensive collection of implementations for each \\nabstraction. These components are designed to be modular \\nand user -friendly, making them useful whethe r you are \\nutilizing the entire LangChain framework or not.  \\nOff-the-Shelf Chains: LangChain offers pre -configured \\nchains, which are structured assemblies of components \\ntailored to accomplish specific high -level tasks. These pre -\\ndefined chains streamline the initial setup process and serve as \\nan ideal starting point  for your projects. The MindGuide Bot \\nuses below components from LangChain . \\nA. ChatModel  \\nWithin LangChain, a ChatModel is a specific kind of \\nlanguage model crafted to manage conversational \\ninteractions. Unlike traditional language models that take one \\nstring as input and generate a single string as output, \\nChatModels operate with a list of mes sages as input, \\ngenerating a message as output.  \\nEach message in the list has two parts: the content and the \\nrole. The content is the actual text or substance of the message, \\nwhile the role denotes the role or source of the message (such \\nas \"User,\" \"Assistant,\" \"System,\" etc.).  \\nThis approach with ChatModels opens the door to more \\ndynamic and interactive conversations with the language \\nmodel. It empowers the creation of chatbot applications, \\ncustomer support systems, or any other application involving \\nmulti -turn conversations. We utilized the ChatOpenAI \\nChatModel to create MindGuide chatbots specifically \\ndesigned to function as mental health therapists. In our \\ninteraction with OpenAI, we opted for an OpenAI API key to \\nengage with the ChatGpt3 turbo model and utilized a \\ntemperature value of 0.5. The steps to create an OpenAI API \\nkey are outlined [ 9].  B. Message  \\nIn the context of LangChain, messages  [10] refer to a list of \\nmessages that are used as input when interacting with a \\nChatModel. Each message in the list represents a specific turn \\nor exchange in a conversation.  Each message in the messages \\nlist typically consists of two components:  \\n content: This represents the actual text or content of \\nthe message. It can be a user query, a system \\ninstruction, or any other relevant information.  \\n role: This represents the role or source of the \\nmessage. It defines who is speaking or generating \\nthe message. Common roles include \"User\", \\n\"Assistant\", \"System\", or any other custom role you \\ndefine . \\nThe chat model interface is based around messages rather \\nthan raw text. The types of messages supported in LangChain \\nare Systen Message, HumanMessage, and AIMessage . \\nSystemMessage  is the ChatMessage coming from the system  \\nin its LangChain template  as illustrated in Figure 1. Human \\nMessage  is a ChatMessage coming from a human/user.  \\nAIMessage is a ChatMessage  coming from an AI/assistant as \\nillustrated in Figure 2 .  \\n \\n                   Figure 1. A System Message illustration   You are a compassionate and experienced mental \\nhealth therapist with a proven track record of \\nhelping patients overcome anxiety and other mental \\nhealth challenges. Your primary objective is to \\nsupport the patient in addressing their concerns \\nand guiding th em towards positive change. In this \\ninteractive therapy session, you will engage with \\nthe patient by asking open -ended questions, \\nactively listening to their responses, and providing \\nempathetic feedback. Your approach is \\ncollaborative, and you strive to cr eate a safe and \\nnon-judgmental space for the patient to share their \\nthoughts and feelings.  \\nAs the patient shares their struggles, you will \\nprovide insightful guidance and evidence -based \\nstrategies tailored to their unique needs. You may \\nalso offer practical exercises or resources to help \\nthem manage their symptoms and improve their \\nmental wellbe ing. When necessary, you will gently \\nredirect the conversation back to the patient\\'s \\nprimary concerns related to anxiety, mental health, \\nor family issues. This ensures that each session is \\nproductive and focused on addressing the most \\npressing issues. Thro ughout the session, you \\nremain mindful of the patient\\'s emotional state and \\nadjust your approach accordingly.  \\nYou recognize that everyone\\'s journey is \\ndifferent, and that progress can be incremental.  \\nBy building trust and fostering a strong \\ntherapeutic relationship, you empower the patient \\nto take ownership of their growth and development. \\nAt the end of the session, you will summarize key \\npoints from your discussion, highlighting the \\npatient\\'s strength s and areas for improvement.  \\nTogether, you will set achievable goals for future \\nsessions, reinforcing a sense of hope and \\nmotivation. Your ultimate goal is to equip the \\npatient with the tools and skills needed to navigate \\nlife\\'s challenges with confidence and resilience . ')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-Query Retriever\n",
    "# Distance-based vector database retrieval represents queries in high-dimensional space and finds similar embedded documents based on \"distance\". \n",
    "# However, retrieval results may vary with subtle changes in query wording or if the embeddings do not accurately capture the data's semantics.\n",
    "\n",
    "# The `MultiQueryRetriever` addresses this by using an LLM to generate multiple queries from different perspectives for a given user input query. \n",
    "# For each query, it retrieves a set of relevant documents and then takes the unique union of these results to form a larger set of potentially relevant documents. \n",
    "# By generating multiple perspectives on the same question, the `MultiQueryRetriever` can potentially overcome some limitations of distance-based retrieval, resulting in a richer and more diverse set of results.\n",
    "\n",
    "\n",
    "# A PDF document has been prepared to demonstrate this Multi-Query Retriever.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"langchain_paper.pdf\")\n",
    "pdf_data = loader.load()\n",
    "\n",
    "# pdf_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document and store the embeddings into a vector database.\n",
    "\n",
    "# Split\n",
    "chunks_pdf = text_splitter(pdf_data, chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# Store in Vector DB (ChromaDB)\n",
    "ids = vectordb.get()[\"ids\"]\n",
    "# ids\n",
    "# vectordb.delete(ids=ids) # You must delete existing entries before adding new ones with same IDs\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=chunks_pdf, embedding=HFembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb.get()[\"ids\"]  # Verify new entries have been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bf91350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "query = \"What does the paper say about langchain?\"\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b8f780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95d86be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What information does the paper provide regarding LangChain?', '', 'What are the insights on LangChain mentioned in the paper?', '', 'What details about LangChain are discussed in the paper?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 5, 'source': 'langchain_paper.pdf'}, page_content='[11] LangChains Large Language Model Chain,  \\nhttps://python.langchain.com/docs/modules/chains/foundational/llm_c\\nhain (accessed Nov. 29, 2023).  \\n[12] Streamlit, https://streamlit.io/  (accessed Nov. 29, 2023).'),\n",
       " Document(metadata={'page': 1, 'source': 'langchain_paper.pdf'}, page_content='LangChain helps us to unlock the ability to harness the \\nLLMs immense potential in tasks such as document analysis, \\nchatbot development, code analysis, and countless other \\napplications. Whether your desire is to unlock deeper natural \\nlanguage understanding , enhance data, or circumvent \\nlanguage barriers through translation, LangChain is ready to \\nprovide the tools and programming support you need to do \\nwithout it that it is not only difficult but also fresh for you . Its'),\n",
       " Document(metadata={'page': 5, 'source': 'langchain_paper.pdf'}, page_content='Nov. 29, 2023).  [7] LangChains Prompt, https://python.langchain.com/docs/modules  \\n/model_io/prompts/ (accessed Nov. 29, 2023).  \\n[8] LangChains Chain s, https://python.langchain.com/docs/modules  \\n/chain s (accessed Nov. 29, 2023).  \\n[9] OpenAI, https://platform.openai.com/docs/quickstart?context=python \\n(accessed Nov. 29, 2023).  \\n[10] LangChains Message Prompt Template,  \\nhttps://python.langchain.com/docs/modules/model_io/prompts/m essa\\nge_prompt s (accessed Nov. 29, 2023).'),\n",
       " Document(metadata={'page': 1, 'source': 'langchain_paper.pdf'}, page_content='core functionalities encompass:  \\n1. Context -Aware Capabilities: LangChain facilitates the \\ndevelopment of applications that are inherently \\ncontext -aware. This means that these applications can \\nconnect to a language model and draw from various \\nsources of context, such as prompt instructions, a  few-\\nshot examples, or existing content, to ground their \\nresponses effectively.  \\n2. Reasoning Abilities: LangChain equips applications \\nwith the capacity to reason effectively. By relying on a'),\n",
       " Document(metadata={'page': 3, 'source': 'langchain_paper.pdf'}, page_content=\"infrastructure is established by  creating  \\nChatMessage and prompt templates for optimal \\nchatbot engagement.  \\n LLMChain and LLM Model Interaction:  To \\nfacilitate interactions with the large language \\nmodel (LLM), a specialized component called \\nLLMChain is constructed. The LLMChain acts \\nas a conduit for managing the flow of \\nconversation between the chatbot and the LLM \\nmodel, in this case, GPT -4.  The LLMChain handle s both the user's queries\"),\n",
       " Document(metadata={'page': 3, 'source': 'langchain_paper.pdf'}, page_content='their mental health questions, kicking off a series of \\ninteractions with the LangChain framework. This is where the \\nmagic happens  LangChain acts as the brain behind the \\nchatbot, working through various components like chat \\nmessage templates and a memory concept to create a \\npersonalized and responsive support system.  Each step is \\nbroken down.  \\nStep 1.  User Interface:  Developed using the Streamlit \\nframework, the user interface welcomes users with a'),\n",
       " Document(metadata={'page': 1, 'source': 'langchain_paper.pdf'}, page_content='helping patients overcome anxiety and other mental \\nhealth challenges. Your primary objective is to \\nsupport the patient in addressing their concerns \\nand guiding th em towards positive change. In this \\ninteractive therapy session, you will engage with \\nthe patient by asking open -ended questions, \\nactively listening to their responses, and providing \\nempathetic feedback. Your approach is \\ncollaborative, and you strive to cr eate a safe and \\nnon-judgmental space for the patient to share their'),\n",
       " Document(metadata={'page': 2, 'source': 'langchain_paper.pdf'}, page_content='LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or \\nincorporated seamlessly into a chain.  \\nA memory system must support two fundamental \\nactions: reading and writing. Remember that each chain has \\nsome fundamental execution mechanism that requires \\nspecific inputs. Some of these inputs are provided directly by \\nthe user, while others may be retrieve d from memory. In a')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Querying Retriever\n",
    "# A Self-Querying Retriever, as the name suggests, has the ability to query itself. \n",
    "# Specifically, given a natural language query, the retriever uses a query-constructing LLM chain to generate a structured query. It then applies this structured query to its underlying vector store. \n",
    "# This enables the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but also to extract and apply filters based on the metadata of those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5b02f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Self-Querying Retriever\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from lark import lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "862a2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docs for demonstration\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7292b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can instantiate your retriever. \n",
    "# To do this, you'll need to provide some upfront information about the metadata fields that your documents support and a brief description of the document contents.\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6ffb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=docs, embedding=HFembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548890a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfQueryRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000168540A6A90>, query_constructor=RunnableBinding(bound=FewShotPromptTemplate(input_variables=['query'], examples=[{'i': 1, 'data_source': '```json\\n{{\\n    \"content\": \"Lyrics of a song\",\\n    \"attributes\": {{\\n        \"artist\": {{\\n            \"type\": \"string\",\\n            \"description\": \"Name of the song artist\"\\n        }},\\n        \"length\": {{\\n            \"type\": \"integer\",\\n            \"description\": \"Length of the song in seconds\"\\n        }},\\n        \"genre\": {{\\n            \"type\": \"string\",\\n            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n        }}\\n    }}\\n}}\\n```', 'user_query': 'What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre', 'structured_request': '```json\\n{{\\n    \"query\": \"teenager love\",\\n    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"\\n}}\\n```'}, {'i': 2, 'data_source': '```json\\n{{\\n    \"content\": \"Lyrics of a song\",\\n    \"attributes\": {{\\n        \"artist\": {{\\n            \"type\": \"string\",\\n            \"description\": \"Name of the song artist\"\\n        }},\\n        \"length\": {{\\n            \"type\": \"integer\",\\n            \"description\": \"Length of the song in seconds\"\\n        }},\\n        \"genre\": {{\\n            \"type\": \"string\",\\n            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"\\n        }}\\n    }}\\n}}\\n```', 'user_query': 'What are songs that were not published on Spotify', 'structured_request': '```json\\n{{\\n    \"query\": \"\",\\n    \"filter\": \"NO_FILTER\"\\n}}\\n```'}], example_prompt=PromptTemplate(input_variables=['data_source', 'i', 'structured_request', 'user_query'], template='<< Example {i}. >>\\nData Source:\\n{data_source}\\n\\nUser Query:\\n{user_query}\\n\\nStructured Request:\\n{structured_request}\\n'), suffix='<< Example 3. >>\\nData Source:\\n```json\\n{{\\n    \"content\": \"page_content\",\\n    \"attributes\": {{\\n    \"genre\": {{\\n        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",\\n        \"type\": \"string\"\\n    }},\\n    \"year\": {{\\n        \"description\": \"The year the movie was released\",\\n        \"type\": \"integer\"\\n    }},\\n    \"director\": {{\\n        \"description\": \"The name of the movie director\",\\n        \"type\": \"string\"\\n    }},\\n    \"rating\": {{\\n        \"description\": \"A 1-10 rating for the movie\",\\n        \"type\": \"float\"\\n    }}\\n}}\\n}}\\n```\\n\\nUser Query:\\n{query}\\n\\nStructured Request:\\n', prefix='Your goal is to structure the user\\'s query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{{\\n    \"query\": string \\\\ text string to compare to document contents\\n    \"filter\": string \\\\ logical condition statement for filtering documents\\n}}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000016805BFB910>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000016805BFBA50>, model_name='mistralai/mistral-small-3.1-24b-instruct:free', temperature=0.2, openai_api_key='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', openai_api_base='https://openrouter.ai/api/v1', openai_proxy='')\n",
       "| StructuredQueryOutputParser(ast_parse=<bound method Lark.parse of Lark(open('<string>'), parser='lalr', lexer='contextual', ...)>), config={'run_name': 'query_constructor'}), structured_query_translator=<langchain_community.query_constructors.chroma.ChromaTranslator object at 0x0000016854814150>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Self-Querying Retriever\n",
    "\n",
    "document_content_description = \"Brief summary of a movie.\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectordb,\n",
    "    document_content_description=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    document_contents=\"page_content\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "347f38b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}, page_content='Three men walk into the Zone, three men walk out of the Zone'),\n",
       " Document(metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}, page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever.invoke(\"Find sci-fi movies released before 2000 with rating over 7\")\n",
    "\n",
    "# This example only specifies a filter\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c84619d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019}, page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example specifies a query and a filter\n",
    "retriever.invoke(\"Has Greta Gerwig directed any movies about women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "397b3a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}, page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea'),\n",
       " Document(metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}, page_content='Three men walk into the Zone, three men walk out of the Zone')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example specifies a composite filter\n",
    "retriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent Document Retriever\n",
    "# When splitting documents for retrieval, there are often conflicting desires:\n",
    "\n",
    "# You may want to have small documents so that their embeddings can most accurately reflect their meaning. \n",
    "# If the documents are too long, the embeddings can lose meaning.\n",
    "\n",
    "# You want to have long enough documents so that the context of each chunk is retained.\n",
    "# The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent IDs for those chunks and returns those larger documents.\n",
    "\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8aa14f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19402ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=\"splitter_parents\", embedding_function=HFembeddings\n",
    ")\n",
    "\n",
    "#vectordb = Chroma.from_documents(documents=chunks_pdf, embedding=watsonx_embedding())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aad7d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    parent_splitter=parent_splitter,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80951b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_text_splitters.base:Created a chunk of size 223, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 274, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 262, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 282, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 262, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 270, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 224, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 325, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 300, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 216, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 226, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 235, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 300, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 294, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 234, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 321, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 256, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 241, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 248, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 249, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 246, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 211, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 267, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 206, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 694, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 323, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 326, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 296, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 233, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 421, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 243, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 260, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 229, which is longer than the specified 200\n",
      "WARNING:langchain_text_splitters.base:Created a chunk of size 290, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "retriever.add_documents(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42128fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb418b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectordb.similarity_search(\"smoking policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9470d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '507dde79-8e9d-4866-bd0a-a40816070b9f', 'source': 'companypolicies.txt'}, page_content='5.\\tSmoking Policy')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs[0]\n",
    "# print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1faa8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.\tSmoking Policy\n",
      "Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations concerning smoking on company premises. This policy is in place to ensure a safe and healthy environment for all employees, visitors, and the general public.\n",
      "Designated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by appropriate signage. These areas have been chosen to minimize exposure to secondhand smoke and to maintain the overall cleanliness of the premises.\n",
      "Smoking Restrictions: Smoking inside company buildings, offices, meeting rooms, and other enclosed spaces is strictly prohibited. This includes electronic cigarettes and vaping devices.\n",
      "Compliance with Applicable Laws: All employees and visitors must adhere to relevant federal, state, and local smoking laws and regulations.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"smoking policy\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5f42a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'director': 'Christopher Nolan', 'rating': 8.2, 'year': 2010}, page_content='Leo DiCaprio gets lost in a dream within a dream within a dream within a ...'),\n",
       " Document(metadata={'director': 'Christopher Nolan', 'rating': 8.2, 'year': 2010}, page_content='Leo DiCaprio gets lost in a dream within a dream within a dream within a ...'),\n",
       " Document(metadata={'director': 'Christopher Nolan', 'rating': 8.2, 'year': 2010}, page_content='Leo DiCaprio gets lost in a dream within a dream within a dream within a ...'),\n",
       " Document(metadata={'director': 'Christopher Nolan', 'rating': 8.2, 'year': 2010}, page_content='Leo DiCaprio gets lost in a dream within a dream within a dream within a ...')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Query Retriever with a Filter\n",
    "\n",
    "vectordb = Chroma.from_documents(docs, embedding=HFembeddings)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    document_content_description=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=\"page_content\",\n",
    "    \n",
    ")\n",
    "\n",
    "# This example specifies a query with filter\n",
    "retriever.invoke(\n",
    "    \"I want to watch a movie directed by Christopher Nolan\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
